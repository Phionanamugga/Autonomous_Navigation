{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOLvU0L2S68hC7zGTuGLUr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phionanamugga/Autonomous_Navigation/blob/main/ObjectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_sync.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcw_teOI3vL5",
        "outputId": "ca632009-fa07-4409-94ed-d21b840391d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-11 15:18:47--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_sync.zip\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.135.51, 3.5.136.90, 52.219.140.31, ...\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.135.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3103291675 (2.9G) [application/zip]\n",
            "Saving to: ‘2011_10_03_drive_0047_sync.zip’\n",
            "\n",
            "2011_10_03_drive_00 100%[===================>]   2.89G  23.8MB/s    in 2m 5s   \n",
            "\n",
            "2025-08-11 15:20:53 (23.6 MB/s) - ‘2011_10_03_drive_0047_sync.zip’ saved [3103291675/3103291675]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_calib.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SM7RVFf13pQ",
        "outputId": "cabed4c6-6210-46d1-ea62-48402174223e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-11 15:21:06--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_calib.zip\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 52.219.47.231, 52.219.169.113, 3.5.134.58, ...\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|52.219.47.231|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4075 (4.0K) [application/zip]\n",
            "Saving to: ‘2011_10_03_calib.zip’\n",
            "\n",
            "2011_10_03_calib.zi 100%[===================>]   3.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-11 15:21:07 (285 MB/s) - ‘2011_10_03_calib.zip’ saved [4075/4075]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q '*.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcTFponY6yBF",
        "outputId": "c40b613f-0b64-415d-cac4-f1ded75f01eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import cv2"
      ],
      "metadata": {
        "id": "MROwg6e3uSgf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (20, 10)"
      ],
      "metadata": {
        "id": "FDoBig6E29pr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExpcHW5Q3Mwq",
        "outputId": "c96a7f54-cd55-4a44-95b7-28a024bc5324"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-11 15:22:29--  https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_utils.py\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/itberrios/CV_tracking/main/kitti_tracker/kitti_utils.py [following]\n",
            "--2025-08-11 15:22:29--  https://raw.githubusercontent.com/itberrios/CV_tracking/main/kitti_tracker/kitti_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9759 (9.5K) [text/plain]\n",
            "Saving to: ‘kitti_utils.py’\n",
            "\n",
            "kitti_utils.py      100%[===================>]   9.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-11 15:22:29 (135 MB/s) - ‘kitti_utils.py’ saved [9759/9759]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZNOm2_613A27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = r'2011_10_03/2011_10_03_drive_0047_sync'\n",
        "# get RGB camera data\n",
        "left_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_02/data/*png')))\n",
        "right_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_03/data/*png')))\n",
        "\n",
        "# getting LiDAR data\n",
        "bin_paths = sorted(glob(os.path.join(DATA_PATH, 'velodyne_points/data/*bin')))\n",
        "\n",
        "# Get GPS/IMU data\n",
        "oxts_paths = sorted(glob(os.path.join(DATA_PATH, r'oxts/data/*txt')))\n",
        "\n",
        "print(f'Number of RGB images: {len(left_image_paths)}')\n",
        "print(f'Number of RGB images: {len(right_image_paths)}')\n",
        "print(f'Number of LiDAR points: {len(bin_paths)}')\n",
        "print(f'Number of GPS/IMU data: {len(oxts_paths)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUCQmZkl4HBn",
        "outputId": "1887dfda-9a04-4e06-81d1-7cf513287281"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of RGB images: 837\n",
            "Number of RGB images: 837\n",
            "Number of LiDAR points: 837\n",
            "Number of GPS/IMU data: 837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading camera calibration data\n",
        "with open('2011_10_03/calib_cam_to_cam.txt', 'r') as f:\n",
        "    calib = f.readlines()\n",
        "\n",
        "# Get projection matrices (rectified left camera ---> left camera (u,v,z))\n",
        "p_rect_cam2 = np.array([float(x) for x in calib[25].strip().split()[1:]]).reshape(3, 4)\n",
        "\n",
        "# Get rectified rotation matrices (left camera ---> rectified left camera)\n",
        "R_ref_rect2 = np.array([float(x) for x in calib[24].strip().split()[1:]]).reshape(3, 3)\n",
        "\n",
        "# Add (0,0,0) translation and convert to homogeneous coordinates\n",
        "R_ref_rect2 = np.insert(R_ref_rect2, 3, values=[0,0,0], axis=1)\n",
        "R_ref_rect2 = np.insert(R_ref_rect2, 3, values=[0,0,0,1], axis=0)\n",
        "\n",
        "# Get rigid transformation from camera 0 (ref) to camera 2\n",
        "R_2 = np.array([float(x) for x in calib[21].strip().split()[1:]]).reshape(3, 3)\n",
        "t_2 = np.array([float(x) for x in calib[22].strip().split()[1:]]).reshape(3, 1)\n",
        "\n",
        "# Get cam0 to cam2 rigid body transformation in homogeneous coordinates\n",
        "T_Cam0_cam2 = np.insert(np.hstack((R_2, t_2)), 3, values=[0,0,0,1], axis=0)"
      ],
      "metadata": {
        "id": "IVo1Yw7yx_Da"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rid_transformation(calib_path):\n",
        "    with open(calib_path, 'r') as f:\n",
        "        calib = f.readlines()\n",
        "    R = np.array([float(x) for x in calib[1].strip().split()[1:]]).reshape(3, 3)\n",
        "    t = np.array([float(x) for x in calib[2].strip().split()[1:]])[:,None]\n",
        "\n",
        "    T = np.vstack((np.hstack((R, t)), [0, 0, 0, 1]))\n",
        "    return T\n",
        "    T_velo_ref = get_rid_tranformation(r'2011_10_03/calib_velo_to_cam.txt')\n",
        "    T_velo_imu = get_rid_tranformation(r'2011_10_03/calib_velo_to_imu.txt')\n",
        "\n",
        "    print(\"T_velo_ref (LiDAR to Camera):\\n\", T_velo_ref)\n",
        "    print(\"T_velo_imu (LiDAR to IMU):\\n\", T_velo_imu)\n",
        ""
      ],
      "metadata": {
        "id": "XoHN2uojzjbf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_velo_ref"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndHeU6AR0HVP",
        "outputId": "147ee27b-1f17-49d8-df9d-9b27e103129c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.967514e-03, -9.999679e-01, -8.462264e-04, -1.377769e-02],\n",
              "       [-2.771053e-03,  8.241710e-04, -9.999958e-01, -5.542117e-02],\n",
              "       [ 9.999644e-01,  7.969825e-03, -2.764397e-03, -2.918589e-01],\n",
              "       [ 0.000000e+00,  0.000000e+00,  0.000000e+00,  1.000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to load rigid body transformation from calibration file\n",
        "def get_rid_transformation(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Calibration file not found: {file_path}\")\n",
        "    with open(file_path, 'r') as f:\n",
        "        calib = f.readlines()\n",
        "    R, t = None, None\n",
        "    for line in calib:\n",
        "        if line.startswith('R:'):\n",
        "            R = np.array([float(x) for x in line.strip().split()[1:]]).reshape(3, 3)\n",
        "        elif line.startswith('T:') or line.startswith('t:'):\n",
        "            t = np.array([float(x) for x in line.strip().split()[1:]]).reshape(3, 1)\n",
        "    if R is None or t is None:\n",
        "        raise ValueError(f\"Invalid format in {file_path}: 'R:' or 'T:'/'t:' not found\")\n",
        "    T = np.vstack((np.hstack((R, t)), [0, 0, 0, 1]))\n",
        "    return T\n",
        "\n",
        "# Loading camera calibration data\n",
        "calib_file = '2011_10_03/calib_cam_to_cam.txt'\n",
        "if not os.path.exists(calib_file):\n",
        "    raise FileNotFoundError(f\"Calibration file not found: {calib_file}\")\n",
        "with open(calib_file, 'r') as f:\n",
        "    calib = f.readlines()\n",
        "\n",
        "# Get projection matrices (rectified left camera ---> left camera (u,v,z))\n",
        "p_rect_cam2 = np.array([float(x) for x in calib[25].strip().split()[1:]]).reshape(3, 4)\n",
        "\n",
        "# Get rectified rotation matrices (left camera ---> rectified left camera)\n",
        "R_ref_rect2 = np.array([float(x) for x in calib[24].strip().split()[1:]]).reshape(3, 3)\n",
        "\n",
        "# Add (0,0,0) translation and convert to homogeneous coordinates\n",
        "R_ref_rect2 = np.insert(R_ref_rect2, 3, values=[0,0,0], axis=1)\n",
        "R_ref_rect2 = np.insert(R_ref_rect2, 3, values=[0,0,0,1], axis=0)\n",
        "\n",
        "# Get rigid transformation from camera 0 (ref) to camera 2\n",
        "R_2 = np.array([float(x) for x in calib[21].strip().split()[1:]]).reshape(3, 3)\n",
        "t_2 = np.array([float(x) for x in calib[22].strip().split()[1:]]).reshape(3, 1)\n",
        "\n",
        "# Get cam0 to cam2 rigid body transformation in homogeneous coordinates\n",
        "T_Cam0_cam2 = np.vstack((np.hstack((R_2, t_2)), [0,0,0,1]))\n",
        "\n",
        "# Loading LiDAR calibration data\n",
        "T_velo_ref = get_rid_transformation('2011_10_03/calib_velo_to_cam.txt')\n",
        "\n",
        "# Skip loading T_velo_imu since the file is missing and not used in transformations\n",
        "# If needed, uncomment and ensure the file exists:\n",
        "# T_velo_imu = get_rid_transformation('2011_10_03/calib_velo_to_imu.txt')\n",
        "\n",
        "# Transform from Velodyne LiDAR to left color camera (shape 3x4)\n",
        "T_velo_cam2 = p_rect_cam2 @ R_ref_rect2 @ T_Cam0_cam2 @ T_velo_ref\n",
        "\n",
        "# Homogeneous transform from left color camera to Velodyne LiDAR (shape 4x4)\n",
        "T_cam2_velo = np.linalg.inv(np.vstack((T_velo_cam2, [0,0,0,1])))\n",
        "\n",
        "# Print results\n",
        "print(\"T_velo_cam2 (LiDAR to left color camera, 3x4):\\n\", T_velo_cam2)\n",
        "print(\"T_cam2_velo (left color camera to LiDAR, 4x4):\\n\", T_cam2_velo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHO1PVLqBLej",
        "outputId": "353826de-5389-4f17-8cda-3f459276a9db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T_velo_cam2 (LiDAR to left color camera, 3x4):\n",
            " [[ 6.07484390e+02 -7.18537361e+02 -1.01875822e+01 -9.55729189e+01]\n",
            " [ 1.80027463e+02  5.89922104e+00 -7.20148711e+02 -9.34570810e+01]\n",
            " [ 9.99973895e-01  4.85949260e-04 -7.20693419e-03 -2.84637351e-01]]\n",
            "T_cam2_velo (left color camera to LiDAR, 4x4):\n",
            " [[ 5.94945492e-07 -1.00306966e-05  1.00147052e+00  2.84175339e-01]\n",
            " [-1.39105363e-03  1.12417451e-05  8.43041553e-01  1.08064679e-01]\n",
            " [-1.12463249e-05 -1.39101753e-03  2.57260040e-01 -5.78494660e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqAOYxfiB6Z7",
        "outputId": "fa9da535-e0b0-45de-dda4-203b2eac8514"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17521, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 17521 (delta 9), reused 0 (delta 0), pack-reused 17497 (from 4)\u001b[K\n",
            "Receiving objects: 100% (17521/17521), 16.62 MiB | 33.18 MiB/s, done.\n",
            "Resolving deltas: 100% (12005/12005), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r yolov5/requirements.txt"
      ],
      "metadata": {
        "id": "hT15934pCdht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gHRgT8tCkMC",
        "outputId": "e5f65b5e-ce03-4dd9-971b-77fee64922c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2025-8-11 Python-3.11.13 torch-2.6.0+cpu CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 118MB/s] \n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set confidence and IOU thresholds\n",
        "model.conf = 0.25 # confidence threshold(0.1) ,default:0.25\n",
        "model.iou = 0.25 # NMS IOU threshold for NMS(0.45)"
      ],
      "metadata": {
        "id": "V6BM66WEC8_I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detections = model(left_image_paths[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTzt46CCFpNr",
        "outputId": "af813e99-ad5d-4a1b-c4a3-b3f5f346318c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_uvz_centers(image, velo_uvz, bboxes, draw=True):\n",
        "    # Unpack LiDAR camera coordinates\n",
        "    u, v, z = velo_uvz[:, 0], velo_uvz[:, 1], velo_uvz[:, 2]\n",
        "\n",
        "    # Initialize output array: bboxes + 3 columns for (u, v, z)\n",
        "    bboxes_out = np.zeros((bboxes.shape[0], bboxes.shape[1] + 3))\n",
        "    bboxes_out[:, :bboxes.shape[1]] = bboxes  # Copy original bboxes\n",
        "\n",
        "    # Iterate through all detected bounding boxes\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        # Convert bbox coordinates to integers\n",
        "        pt1 = torch.round(bbox[0:2]).to(torch.int).numpy()  # (y_min, x_min)\n",
        "        pt2 = torch.round(bbox[2:4]).to(torch.int).numpy()  # (y_max, x_max)\n",
        "\n",
        "        # Get center location of the object on the image\n",
        "        obj_x_center = (pt1[1] + pt2[1]) / 2  # x center\n",
        "        obj_y_center = (pt1[0] + pt2[0]) / 2  # y center\n",
        "\n",
        "        # Compute distance from LiDAR points to the center\n",
        "        center_delta = np.abs(np.vstack((u, v)).T - np.array([obj_x_center, obj_y_center]))\n",
        "\n",
        "        # Choose coordinate pair with the smallest L2 norm\n",
        "        min_loc = np.argmin(np.linalg.norm(center_delta, axis=1))\n",
        "\n",
        "        # Get LiDAR location in image/camera space\n",
        "        velo_depth = z[min_loc]\n",
        "        uvz_location = np.array([u[min_loc], v[min_loc], velo_depth])\n",
        "\n",
        "        # Add velo projections (u, v, z) to bboxes_out\n",
        "        bboxes_out[i, -3:] = uvz_location\n",
        "\n",
        "        # Draw depth on image at center of each bounding box\n",
        "        if draw:\n",
        "            object_center = (int(np.round(obj_x_center)), int(np.round(obj_y_center)))\n",
        "            cv2.putText(\n",
        "                image,\n",
        "                '{:.2f} m'.format(velo_depth),\n",
        "                object_center,  # Center of bounding box\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.5,\n",
        "                (255, 0, 0),  # Blue color in BGR\n",
        "                2,\n",
        "                cv2.LINE_AA\n",
        "            )\n",
        "\n",
        "    return bboxes_out"
      ],
      "metadata": {
        "id": "Im4QcTd_F9kT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_detection_coordinates(image, bin_path, draw_boxes=True, draw_depth =True):\n",
        "    # compute detections in the left image\n",
        "    detections = model(left_image_paths[0])\n",
        "\n",
        "    # draw boxes on images\n",
        "    if draw_boxes:\n",
        "        detections.show()\n",
        "\n",
        "\n",
        "    # get bounding box locations\n",
        "    bboxes = detections.xyxy[0].cpu().numpy()\n",
        "\n",
        "    velo_uvz = project_velobin2uvz(bin_path,\n",
        "                                   T_velo_cam2,\n",
        "                                   left_image,\n",
        "                                   remove_plane = True)\n",
        "\n",
        "    # get uvz centers for detected objects\n",
        "    bboxes_out = get_uvz_centers(image, velo_uvz, bboxes, draw=draw_depth)\n",
        "\n",
        "    return bboxes, velo_uvz"
      ],
      "metadata": {
        "id": "yS4G_pjVZpKe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymap3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br3u3EnfhVkD",
        "outputId": "c3abe270-021f-4bd1-a85d-5fdd7fc55463"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymap3d\n",
            "  Downloading pymap3d-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading pymap3d-3.2.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymap3d\n",
            "Successfully installed pymap3d-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymap3d as pm\n",
        "def imu2geodetic(x,y,z, lat0, lon0, alt0, heading0):\n",
        "\n",
        "  # convert to RAE\n",
        "  rng = np.sqrt(x**2 + y**2 + z**2)\n",
        "  az = np.degrees(np.arctan2(y,x)) + np.degrees(heading0)\n",
        "  el = np.degrees(np.arctan2(np.sqrt(x**2 + y**2), z)) + 90\n",
        "\n",
        "  # convert to geodetic\n",
        "  lla = pm.geodetic2ned(az, el, rng,lat0, lon0, alt0)\n",
        "\n",
        "  # convert to numpy array\n",
        "  lla = np.vstack((lla[0], lla[1], lla[2])).T\n",
        "  return lla\n",
        ""
      ],
      "metadata": {
        "id": "Gbxsfa8phiyY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the pipeline\n",
        "index = 10\n",
        "left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]),cv2.COLOR_BGR2RGB)\n",
        "bin_path = bin_paths[index]\n",
        "oxts_frame = get_oxts(oxts_paths[index])\n",
        "\n",
        "# get detections and object centers in uvz\n",
        "bboxes, velo_uvz = get_detection_coordinates(left_image, bin_path, draw_boxes=True, draw_depth=True)\n",
        "\n",
        "# get transformed coordinates of object centers\n",
        "uvz = bboxes[:, -3:]\n",
        "\n",
        "# transform to (u,v,z)\n",
        "imu_uvz = transform_uvz(uvz, T_cam2_imu)\n",
        "\n",
        "# get Lat/Long on each detected objects\n",
        "lat0 = oxts_frame[0]\n",
        "lon0 = oxts_frame[1]\n",
        "alt0 = oxts_frame[2]\n",
        "heading0 = oxts_frame[5]\n",
        "\n"
      ],
      "metadata": {
        "id": "N7rYN23ZnlCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mmcv\n",
        "print(mmcv.__version__)  # Should print 2.1.0\n",
        "import mmdet\n",
        "import mmdet3d\n",
        "print(mmdet.__version__, mmdet3d.__version__)  # Should print 3.3.0, 1.4.0"
      ],
      "metadata": {
        "id": "lt_AhmoaXI1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mmdet3d.datasets import build_dataset\n",
        "cfg.data.train.data_root = './data/kitti/'\n",
        "dataset = build_dataset(cfg.data.train)"
      ],
      "metadata": {
        "id": "3P564p55Zc6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Nxv3MT5BccE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.15.2', 'deeplabv3_resnet101', pretrained=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "xlrJ5l5tOq1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                               std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "def segment_image(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(img).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)['out'][0]\n",
        "    sem_prob = torch.softmax(output, dim=0).cpu().numpy()  # shape: [21, H, W]\n",
        "    return sem_prob\n"
      ],
      "metadata": {
        "id": "s9b5yALJOxEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def project_lidar_to_image(points, calib):\n",
        "    # points: Nx3 array (x,y,z)\n",
        "    # calib: calibration matrix from KITTI\n",
        "    # Return: Nx2 array of pixel coordinates\n",
        "    pts_3d = np.hstack((points, np.ones((points.shape[0],1))))\n",
        "    pts_2d = calib @ pts_3d.T\n",
        "    pts_2d[:2, :] /= pts_2d[2, :]\n",
        "    return pts_2d[:2, :].T  # Nx2 pixel coordinates"
      ],
      "metadata": {
        "id": "sqCt2nLfOyYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}