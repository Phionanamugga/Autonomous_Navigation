{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKxWuBIWmMheGssJqkHZFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phionanamugga/Autonomous_Navigation/blob/main/ObjectDetection2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymap3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPBE8RmjMaa1",
        "outputId": "49c78cb1-9c1c-4ab2-c615-63cf94103cfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymap3d\n",
            "  Downloading pymap3d-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading pymap3d-3.2.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m61.4/64.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymap3d\n",
            "Successfully installed pymap3d-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Advanced LiDAR-Camera Fusion System for Autonomous Driving\n",
        "=======================================================\n",
        "\n",
        "A production-ready implementation featuring:\n",
        "- Modular, extensible architecture\n",
        "- Real-time performance optimization\n",
        "- Comprehensive error handling\n",
        "- Extensive logging and monitoring\n",
        "- Multi-threading support\n",
        "- Advanced calibration and validation\n",
        "- Performance benchmarking\n",
        "- Clean code practices following industry standards\n",
        "\n",
        "Author: Your Name\n",
        "Date: 2025\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import threading\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Union, Any\n",
        "from pathlib import Path\n",
        "import concurrent.futures\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import pymap3d as pm\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.cluster import DBSCAN\n",
        "import yaml"
      ],
      "metadata": {
        "id": "zRZk3VAIJoeI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('fusion_system.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "Lt6NbhReM8NN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CalibrationData:\n",
        "    \"\"\"Encapsulates all calibration matrices and parameters.\"\"\"\n",
        "    P_rect: np.ndarray  # Projection matrix\n",
        "    R_rect: np.ndarray  # Rectification matrix\n",
        "    T_cam_velo: np.ndarray  # Camera to LiDAR transform\n",
        "    T_velo_cam: np.ndarray  # LiDAR to camera transform\n",
        "    T_cam_imu: np.ndarray  # Camera to IMU transform\n",
        "    camera_intrinsics: Dict[str, float]\n",
        "    distortion_coeffs: np.ndarray\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate calibration data.\"\"\"\n",
        "        self._validate_matrices()\n",
        "\n",
        "    def _validate_matrices(self):\n",
        "        \"\"\"Validate calibration matrix dimensions and properties.\"\"\"\n",
        "        assert self.P_rect.shape == (3, 4), f\"Invalid P_rect shape: {self.P_rect.shape}\"\n",
        "        assert self.R_rect.shape == (4, 4), f\"Invalid R_rect shape: {self.R_rect.shape}\"\n",
        "        assert self.T_velo_cam.shape == (3, 4), f\"Invalid T_velo_cam shape: {self.T_velo_cam.shape}\"\n",
        "        logger.info(\"Calibration matrices validated successfully\")"
      ],
      "metadata": {
        "id": "rKfy2-CWNC3S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Detection:\n",
        "    \"\"\"Represents a single object detection.\"\"\"\n",
        "    bbox: np.ndarray  # [x1, y1, x2, y2]\n",
        "    confidence: float\n",
        "    class_id: int\n",
        "    class_name: str\n",
        "    center_3d: Optional[np.ndarray] = None  # [x, y, z] in camera coordinates\n",
        "    distance: Optional[float] = None\n",
        "    gps_coords: Optional[Tuple[float, float, float]] = None  # (lat, lon, alt)\n",
        "    timestamp: float = field(default_factory=time.time)\n"
      ],
      "metadata": {
        "id": "aec1rm29NG6F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SensorFrame:\n",
        "    \"\"\"Container for synchronized sensor data.\"\"\"\n",
        "    timestamp: float\n",
        "    image: np.ndarray\n",
        "    lidar_points: np.ndarray\n",
        "    gps_data: Dict[str, float]\n",
        "    imu_data: Dict[str, float]\n",
        "    frame_id: int\n"
      ],
      "metadata": {
        "id": "alL064q-NN1j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceMonitor:\n",
        "    \"\"\"Tracks and reports system performance metrics.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'processing_times': [],\n",
        "            'detection_counts': [],\n",
        "            'fusion_accuracy': [],\n",
        "            'memory_usage': [],\n",
        "            'cpu_usage': []\n",
        "        }\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    @contextmanager\n",
        "    def measure_time(self, operation: str):\n",
        "        \"\"\"Context manager to measure execution time.\"\"\"\n",
        "        start = time.time()\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            duration = time.time() - start\n",
        "            self.metrics['processing_times'].append({\n",
        "                'operation': operation,\n",
        "                'duration': duration,\n",
        "                'timestamp': time.time()\n",
        "            })\n",
        "            logger.debug(f\"{operation} took {duration:.4f} seconds\")\n",
        "\n",
        "    def log_detection_count(self, count: int):\n",
        "        \"\"\"Log number of detections per frame.\"\"\"\n",
        "        self.metrics['detection_counts'].append({\n",
        "            'count': count,\n",
        "            'timestamp': time.time()\n",
        "        })\n",
        "\n",
        "    def get_performance_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
        "        processing_times = [m['duration'] for m in self.metrics['processing_times']]\n",
        "        return {\n",
        "            'avg_processing_time': np.mean(processing_times) if processing_times else 0,\n",
        "            'max_processing_time': np.max(processing_times) if processing_times else 0,\n",
        "            'min_processing_time': np.min(processing_times) if processing_times else 0,\n",
        "            'total_frames_processed': len(self.metrics['detection_counts']),\n",
        "            'avg_detections_per_frame': np.mean([m['count'] for m in self.metrics['detection_counts']]) if self.metrics['detection_counts'] else 0,\n",
        "            'uptime': time.time() - self.start_time\n",
        "        }"
      ],
      "metadata": {
        "id": "RgdUYtDPNTXg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigManager:\n",
        "    \"\"\"Manages system configuration.\"\"\"\n",
        "\n",
        "    DEFAULT_CONFIG = {\n",
        "        'detection': {\n",
        "            'model_name': 'yolov5s',\n",
        "            'confidence_threshold': 0.25,\n",
        "            'iou_threshold': 0.25,\n",
        "            'max_detections': 100\n",
        "        },\n",
        "        'fusion': {\n",
        "            'max_distance_threshold': 100.0,  # meters\n",
        "            'point_cluster_eps': 0.5,\n",
        "            'min_cluster_size': 3,\n",
        "            'depth_filter_sigma': 2.0\n",
        "        },\n",
        "        'performance': {\n",
        "            'use_gpu': True,\n",
        "            'num_threads': 4,\n",
        "            'batch_size': 1\n",
        "        },\n",
        "        'logging': {\n",
        "            'level': 'INFO',\n",
        "            'save_processed_frames': False\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def __init__(self, config_path: Optional[str] = None):\n",
        "        self.config = self.DEFAULT_CONFIG.copy()\n",
        "        if config_path and os.path.exists(config_path):\n",
        "            self.load_config(config_path)\n",
        "\n",
        "    def load_config(self, config_path: str):\n",
        "        \"\"\"Load configuration from YAML file.\"\"\"\n",
        "        try:\n",
        "            with open(config_path, 'r') as f:\n",
        "                user_config = yaml.safe_load(f)\n",
        "                self._deep_update(self.config, user_config)\n",
        "            logger.info(f\"Configuration loaded from {config_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load config from {config_path}: {e}\")\n",
        "\n",
        "    def _deep_update(self, base_dict: Dict, update_dict: Dict):\n",
        "        \"\"\"Recursively update nested dictionary.\"\"\"\n",
        "        for key, value in update_dict.items():\n",
        "            if isinstance(value, dict) and key in base_dict:\n",
        "                self._deep_update(base_dict[key], value)\n",
        "            else:\n",
        "                base_dict[key] = value\n",
        "\n",
        "    def get(self, key_path: str, default=None):\n",
        "        \"\"\"Get configuration value using dot notation.\"\"\"\n",
        "        keys = key_path.split('.')\n",
        "        value = self.config\n",
        "        for key in keys:\n",
        "            if isinstance(value, dict) and key in value:\n",
        "                value = value[key]\n",
        "            else:\n",
        "                return default\n",
        "        return value"
      ],
      "metadata": {
        "id": "6HpeB0HeNazj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Handles loading and preprocessing of KITTI dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str):\n",
        "        self.data_path = Path(data_path)\n",
        "        self._validate_data_structure()\n",
        "        self._load_file_paths()\n",
        "\n",
        "    def _validate_data_structure(self):\n",
        "        \"\"\"Validate KITTI dataset structure.\"\"\"\n",
        "        required_dirs = ['image_02/data', 'velodyne_points/data', 'oxts/data']\n",
        "        for dir_name in required_dirs:\n",
        "            dir_path = self.data_path / dir_name\n",
        "            if not dir_path.exists():\n",
        "                raise FileNotFoundError(f\"Required directory not found: {dir_path}\")\n",
        "        logger.info(\"Data structure validation passed\")\n",
        "\n",
        "    def _load_file_paths(self):\n",
        "        \"\"\"Load all file paths and ensure synchronization.\"\"\"\n",
        "        self.left_image_paths = sorted((self.data_path / 'image_02/data').glob('*.png'))\n",
        "        self.lidar_paths = sorted((self.data_path / 'velodyne_points/data').glob('*.bin'))\n",
        "        self.oxts_paths = sorted((self.data_path / 'oxts/data').glob('*.txt'))\n",
        "\n",
        "        # Ensure all sensors have same number of frames\n",
        "        lengths = [len(self.left_image_paths), len(self.lidar_paths), len(self.oxts_paths)]\n",
        "        if not all(l == lengths[0] for l in lengths):\n",
        "            logger.warning(f\"Sensor data length mismatch: {lengths}\")\n",
        "\n",
        "        self.num_frames = min(lengths)\n",
        "        logger.info(f\"Loaded {self.num_frames} synchronized frames\")\n",
        "\n",
        "    def load_frame(self, frame_idx: int) -> SensorFrame:\n",
        "        \"\"\"Load synchronized sensor data for a specific frame.\"\"\"\n",
        "        if frame_idx >= self.num_frames:\n",
        "            raise IndexError(f\"Frame index {frame_idx} out of range\")\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.cvtColor(cv2.imread(str(self.left_image_paths[frame_idx])), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load LiDAR points\n",
        "        lidar_points = np.fromfile(str(self.lidar_paths[frame_idx]), dtype=np.float32).reshape(-1, 4)\n",
        "\n",
        "        # Load GPS/IMU data\n",
        "        gps_imu_data = self._parse_oxts_data(str(self.oxts_paths[frame_idx]))\n",
        "\n",
        "        return SensorFrame(\n",
        "            timestamp=time.time(),\n",
        "            image=image,\n",
        "            lidar_points=lidar_points,\n",
        "            gps_data=gps_imu_data['gps'],\n",
        "            imu_data=gps_imu_data['imu'],\n",
        "            frame_id=frame_idx\n",
        "        )\n",
        "\n",
        "    def _parse_oxts_data(self, oxts_path: str) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Parse OXTS GPS/IMU data file.\"\"\"\n",
        "        with open(oxts_path, 'r') as f:\n",
        "            values = [float(x) for x in f.readline().strip().split()]\n",
        "\n",
        "        return {\n",
        "            'gps': {\n",
        "                'lat': values[0],\n",
        "                'lon': values[1],\n",
        "                'alt': values[2],\n",
        "                'roll': values[3],\n",
        "                'pitch': values[4],\n",
        "                'yaw': values[5]\n",
        "            },\n",
        "            'imu': {\n",
        "                'vn': values[6],  # velocity north\n",
        "                've': values[7],  # velocity east\n",
        "                'vf': values[8],  # velocity forward\n",
        "                'vl': values[9],  # velocity left\n",
        "                'vu': values[10],  # velocity up\n",
        "                'ax': values[11],  # acceleration x\n",
        "                'ay': values[12],  # acceleration y\n",
        "                'az': values[13],  # acceleration z\n",
        "                'af': values[14],  # acceleration forward\n",
        "                'al': values[15],  # acceleration left\n",
        "                'au': values[16],  # acceleration up\n",
        "                'wx': values[17],  # angular rate x\n",
        "                'wy': values[18],  # angular rate y\n",
        "                'wz': values[19]   # angular rate z\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "dhK_2fgWNhg9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CalibrationLoader:\n",
        "    \"\"\"Loads and manages calibration data.\"\"\"\n",
        "\n",
        "    def __init__(self, calib_dir: str):\n",
        "        self.calib_dir = Path(calib_dir)\n",
        "\n",
        "    def load_calibration(self) -> CalibrationData:\n",
        "        \"\"\"Load all calibration matrices.\"\"\"\n",
        "        try:\n",
        "            # Load camera calibration\n",
        "            cam_calib = self._load_camera_calibration()\n",
        "\n",
        "            # Load LiDAR calibration\n",
        "            lidar_calib = self._load_lidar_calibration()\n",
        "\n",
        "            # Compute transformation matrices\n",
        "            T_velo_cam = self._compute_velo_to_cam_transform(cam_calib, lidar_calib)\n",
        "            T_cam_velo = np.linalg.inv(np.vstack([T_velo_cam, [0, 0, 0, 1]]))\n",
        "\n",
        "            return CalibrationData(\n",
        "                P_rect=cam_calib['P_rect'],\n",
        "                R_rect=cam_calib['R_rect'],\n",
        "                T_cam_velo=T_cam_velo,\n",
        "                T_velo_cam=T_velo_cam,\n",
        "                T_cam_imu=np.eye(4),  # Placeholder\n",
        "                camera_intrinsics=cam_calib['intrinsics'],\n",
        "                distortion_coeffs=cam_calib['distortion']\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load calibration data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_camera_calibration(self) -> Dict:\n",
        "        \"\"\"Load camera calibration matrices.\"\"\"\n",
        "        calib_file = self.calib_dir / 'calib_cam_to_cam.txt'\n",
        "        with open(calib_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Parse projection matrix (P_rect_02)\n",
        "        P_rect = np.array([float(x) for x in lines[25].strip().split()[1:]]).reshape(3, 4)\n",
        "\n",
        "        # Parse rectification matrix (R_rect_00)\n",
        "        R_rect_3x3 = np.array([float(x) for x in lines[24].strip().split()[1:]]).reshape(3, 3)\n",
        "        R_rect = np.eye(4)\n",
        "        R_rect[:3, :3] = R_rect_3x3\n",
        "\n",
        "        # Extract intrinsics from projection matrix\n",
        "        fx, fy = P_rect[0, 0], P_rect[1, 1]\n",
        "        cx, cy = P_rect[0, 2], P_rect[1, 2]\n",
        "\n",
        "        return {\n",
        "            'P_rect': P_rect,\n",
        "            'R_rect': R_rect,\n",
        "            'intrinsics': {'fx': fx, 'fy': fy, 'cx': cx, 'cy': cy},\n",
        "            'distortion': np.zeros(5)  # Assuming rectified images\n",
        "        }\n",
        "\n",
        "    def _load_lidar_calibration(self) -> Dict:\n",
        "        \"\"\"Load LiDAR calibration matrices.\"\"\"\n",
        "        calib_file = self.calib_dir / 'calib_velo_to_cam.txt'\n",
        "        with open(calib_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        R = np.array([float(x) for x in lines[1].strip().split()[1:]]).reshape(3, 3)\n",
        "        t = np.array([float(x) for x in lines[2].strip().split()[1:]]).reshape(3, 1)\n",
        "\n",
        "        return {'R': R, 't': t}\n",
        "\n",
        "    def _compute_velo_to_cam_transform(self, cam_calib: Dict, lidar_calib: Dict) -> np.ndarray:\n",
        "        \"\"\"Compute complete transformation from LiDAR to camera coordinates.\"\"\"\n",
        "        # LiDAR to camera coordinate system\n",
        "        T_velo_cam_0 = np.hstack([lidar_calib['R'], lidar_calib['t']])\n",
        "        T_velo_cam_0 = np.vstack([T_velo_cam_0, [0, 0, 0, 1]])\n",
        "\n",
        "        # Apply rectification and projection\n",
        "        T_velo_cam = cam_calib['P_rect'] @ cam_calib['R_rect'] @ T_velo_cam_0\n",
        "\n",
        "        return T_velo_cam[:3, :]"
      ],
      "metadata": {
        "id": "vYczWvWGNqT-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetector:\n",
        "    \"\"\"Handles object detection using YOLO.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigManager):\n",
        "        self.config = config\n",
        "        self.model = self._load_model()\n",
        "        self._configure_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load YOLO model.\"\"\"\n",
        "        model_name = self.config.get('detection.model_name', 'yolov5s')\n",
        "        try:\n",
        "            model = torch.hub.load('ultralytics/yolov5', model_name, pretrained=True)\n",
        "            logger.info(f\"Loaded model: {model_name}\")\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model {model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _configure_model(self):\n",
        "        \"\"\"Configure model parameters.\"\"\"\n",
        "        self.model.conf = self.config.get('detection.confidence_threshold', 0.25)\n",
        "        self.model.iou = self.config.get('detection.iou_threshold', 0.25)\n",
        "        self.model.max_det = self.config.get('detection.max_detections', 100)\n",
        "\n",
        "        # Use GPU if available and configured\n",
        "        if self.config.get('performance.use_gpu', True) and torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "            logger.info(\"Using GPU acceleration\")\n",
        "\n",
        "    def detect(self, image: np.ndarray) -> List[Detection]:\n",
        "        \"\"\"Perform object detection on image.\"\"\"\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                results = self.model(image)\n",
        "\n",
        "            detections = []\n",
        "            for detection in results.xyxy[0].cpu().numpy():\n",
        "                x1, y1, x2, y2, conf, class_id = detection\n",
        "                class_name = self.model.names[int(class_id)]\n",
        "\n",
        "                detections.append(Detection(\n",
        "                    bbox=np.array([x1, y1, x2, y2]),\n",
        "                    confidence=float(conf),\n",
        "                    class_id=int(class_id),\n",
        "                    class_name=class_name\n",
        "                ))\n",
        "\n",
        "            logger.debug(f\"Detected {len(detections)} objects\")\n",
        "            return detections\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Detection failed: {e}\")\n",
        "            return []\n",
        "\n"
      ],
      "metadata": {
        "id": "XnMqdIhlNwoY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LiDARProcessor:\n",
        "    \"\"\"Processes LiDAR point clouds.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigManager):\n",
        "        self.config = config\n",
        "        self.max_distance = config.get('fusion.max_distance_threshold', 100.0)\n",
        "\n",
        "    def filter_points(self, points: np.ndarray, remove_ground: bool = True) -> np.ndarray:\n",
        "        \"\"\"Filter LiDAR points based on distance and ground removal.\"\"\"\n",
        "        # Remove points with zero intensity (common filtering)\n",
        "        if points.shape[1] > 3:\n",
        "            points = points[points[:, 3] > 0]\n",
        "\n",
        "        # Distance filtering\n",
        "        distances = np.linalg.norm(points[:, :3], axis=1)\n",
        "        points = points[distances <= self.max_distance]\n",
        "\n",
        "        # Ground removal using simple height threshold\n",
        "        if remove_ground:\n",
        "            points = points[points[:, 2] > -1.5]  # Remove points below -1.5m\n",
        "\n",
        "        logger.debug(f\"Filtered to {len(points)} LiDAR points\")\n",
        "        return points\n",
        "\n",
        "    def project_to_image(self, points: np.ndarray, T_velo_cam: np.ndarray,\n",
        "                        image_shape: Tuple[int, int]) -> np.ndarray:\n",
        "        \"\"\"Project 3D LiDAR points to 2D image coordinates.\"\"\"\n",
        "        # Convert to homogeneous coordinates\n",
        "        points_hom = np.hstack([points[:, :3], np.ones((len(points), 1))])\n",
        "\n",
        "        # Transform to camera coordinates\n",
        "        points_cam = (T_velo_cam @ points_hom.T).T\n",
        "\n",
        "        # Remove points behind camera\n",
        "        valid_mask = points_cam[:, 2] > 0\n",
        "        points_cam = points_cam[valid_mask]\n",
        "\n",
        "        # Project to image plane\n",
        "        u = points_cam[:, 0] / points_cam[:, 2]\n",
        "        v = points_cam[:, 1] / points_cam[:, 2]\n",
        "\n",
        "        # Filter points within image bounds\n",
        "        h, w = image_shape[:2]\n",
        "        valid_mask = (u >= 0) & (u < w) & (v >= 0) & (v < h)\n",
        "\n",
        "        points_proj = np.column_stack([u[valid_mask], v[valid_mask], points_cam[valid_mask, 2]])\n",
        "\n",
        "        logger.debug(f\"Projected {len(points_proj)} points to image\")\n",
        "        return points_proj\n",
        "\n",
        "    def cluster_points(self, points: np.ndarray) -> List[np.ndarray]:\n",
        "        \"\"\"Cluster LiDAR points using DBSCAN.\"\"\"\n",
        "        if len(points) < 3:\n",
        "            return [points] if len(points) > 0 else []\n",
        "\n",
        "        clustering = DBSCAN(\n",
        "            eps=self.config.get('fusion.point_cluster_eps', 0.5),\n",
        "            min_samples=self.config.get('fusion.min_cluster_size', 3)\n",
        "        )\n",
        "\n",
        "        cluster_labels = clustering.fit_predict(points[:, :2])  # Cluster based on x, y\n",
        "\n",
        "        clusters = []\n",
        "        for label in set(cluster_labels):\n",
        "            if label != -1:  # Ignore noise points\n",
        "                cluster_points = points[cluster_labels == label]\n",
        "                clusters.append(cluster_points)\n",
        "\n",
        "        logger.debug(f\"Found {len(clusters)} point clusters\")\n",
        "        return clusters"
      ],
      "metadata": {
        "id": "jaBm3c8jOAAe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FusionEngine:\n",
        "    \"\"\"Core fusion engine that combines camera and LiDAR data.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigManager, calibration: CalibrationData):\n",
        "        self.config = config\n",
        "        self.calibration = calibration\n",
        "        self.lidar_processor = LiDARProcessor(config)\n",
        "\n",
        "    def fuse_detections(self, detections: List[Detection], lidar_points: np.ndarray,\n",
        "                       image_shape: Tuple[int, int]) -> List[Detection]:\n",
        "        \"\"\"Fuse object detections with LiDAR data.\"\"\"\n",
        "        if not detections:\n",
        "            return detections\n",
        "\n",
        "        # Filter and project LiDAR points\n",
        "        filtered_points = self.lidar_processor.filter_points(lidar_points)\n",
        "        projected_points = self.lidar_processor.project_to_image(\n",
        "            filtered_points, self.calibration.T_velo_cam, image_shape\n",
        "        )\n",
        "\n",
        "        if len(projected_points) == 0:\n",
        "            logger.warning(\"No LiDAR points projected to image\")\n",
        "            return detections\n",
        "\n",
        "        # Associate LiDAR points with detections\n",
        "        fused_detections = []\n",
        "        for detection in detections:\n",
        "            fused_detection = self._associate_lidar_with_detection(\n",
        "                detection, projected_points, filtered_points\n",
        "            )\n",
        "            fused_detections.append(fused_detection)\n",
        "\n",
        "        return fused_detections\n",
        "\n",
        "    def _associate_lidar_with_detection(self, detection: Detection,\n",
        "                                       projected_points: np.ndarray,\n",
        "                                       world_points: np.ndarray) -> Detection:\n",
        "        \"\"\"Associate LiDAR points with a single detection.\"\"\"\n",
        "        x1, y1, x2, y2 = detection.bbox\n",
        "\n",
        "        # Find points within bounding box\n",
        "        mask = ((projected_points[:, 0] >= x1) & (projected_points[:, 0] <= x2) &\n",
        "                (projected_points[:, 1] >= y1) & (projected_points[:, 1] <= y2))\n",
        "\n",
        "        if not np.any(mask):\n",
        "            # If no points in bbox, find closest point to center\n",
        "            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "            distances = np.sqrt((projected_points[:, 0] - center_x)**2 +\n",
        "                              (projected_points[:, 1] - center_y)**2)\n",
        "            closest_idx = np.argmin(distances)\n",
        "\n",
        "            detection.center_3d = world_points[closest_idx, :3]\n",
        "            detection.distance = projected_points[closest_idx, 2]\n",
        "        else:\n",
        "            # Use median depth of points in bounding box for robustness\n",
        "            bbox_points = projected_points[mask]\n",
        "            bbox_world_points = world_points[mask]\n",
        "\n",
        "            median_depth_idx = np.argsort(bbox_points[:, 2])[len(bbox_points) // 2]\n",
        "            detection.center_3d = bbox_world_points[median_depth_idx, :3]\n",
        "            detection.distance = bbox_points[median_depth_idx, 2]\n",
        "\n",
        "        return detection\n",
        "\n",
        "    def transform_to_global(self, detections: List[Detection],\n",
        "                           gps_data: Dict[str, float]) -> List[Detection]:\n",
        "        \"\"\"Transform detection coordinates to global GPS coordinates.\"\"\"\n",
        "        lat0, lon0, alt0 = gps_data['lat'], gps_data['lon'], gps_data['alt']\n",
        "        heading0 = gps_data['yaw']\n",
        "\n",
        "        for detection in detections:\n",
        "            if detection.center_3d is not None:\n",
        "                x, y, z = detection.center_3d\n",
        "\n",
        "                # Convert to range, azimuth, elevation\n",
        "                rng = np.sqrt(x**2 + y**2 + z**2)\n",
        "                az = np.degrees(np.arctan2(y, x)) + np.degrees(heading0)\n",
        "                el = np.degrees(np.arctan2(np.sqrt(x**2 + y**2), z)) + 90\n",
        "\n",
        "                # Convert to geodetic coordinates\n",
        "                lat, lon, alt = pm.aer2geodetic(az, el, rng, lat0, lon0, alt0)\n",
        "                detection.gps_coords = (lat, lon, alt)\n",
        "\n",
        "        return detections"
      ],
      "metadata": {
        "id": "wjbmnRbuOHQz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FusionEngine:\n",
        "    \"\"\"Core fusion engine that combines camera and LiDAR data.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfigManager, calibration: CalibrationData):\n",
        "        self.config = config\n",
        "        self.calibration = calibration\n",
        "        self.lidar_processor = LiDARProcessor(config)\n",
        "\n",
        "    def fuse_detections(self, detections: List[Detection], lidar_points: np.ndarray,\n",
        "                       image_shape: Tuple[int, int]) -> List[Detection]:\n",
        "        \"\"\"Fuse object detections with LiDAR data.\"\"\"\n",
        "        if not detections:\n",
        "            return detections\n",
        "\n",
        "        # Filter and project LiDAR points\n",
        "        filtered_points = self.lidar_processor.filter_points(lidar_points)\n",
        "        projected_points = self.lidar_processor.project_to_image(\n",
        "            filtered_points, self.calibration.T_velo_cam, image_shape\n",
        "        )\n",
        "\n",
        "        if len(projected_points) == 0:\n",
        "            logger.warning(\"No LiDAR points projected to image\")\n",
        "            return detections\n",
        "\n",
        "        # Associate LiDAR points with detections\n",
        "        fused_detections = []\n",
        "        for detection in detections:\n",
        "            fused_detection = self._associate_lidar_with_detection(\n",
        "                detection, projected_points, filtered_points\n",
        "            )\n",
        "            fused_detections.append(fused_detection)\n",
        "\n",
        "        return fused_detections\n",
        "\n",
        "    def _associate_lidar_with_detection(self, detection: Detection,\n",
        "                                       projected_points: np.ndarray,\n",
        "                                       world_points: np.ndarray) -> Detection:\n",
        "        \"\"\"Associate LiDAR points with a single detection.\"\"\"\n",
        "        x1, y1, x2, y2 = detection.bbox\n",
        "\n",
        "        # Find points within bounding box\n",
        "        mask = ((projected_points[:, 0] >= x1) & (projected_points[:, 0] <= x2) &\n",
        "                (projected_points[:, 1] >= y1) & (projected_points[:, 1] <= y2))\n",
        "\n",
        "        if not np.any(mask):\n",
        "            # If no points in bbox, find closest point to center\n",
        "            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "            distances = np.sqrt((projected_points[:, 0] - center_x)**2 +\n",
        "                              (projected_points[:, 1] - center_y)**2)\n",
        "            closest_idx = np.argmin(distances)\n",
        "\n",
        "            detection.center_3d = world_points[closest_idx, :3]\n",
        "            detection.distance = projected_points[closest_idx, 2]\n",
        "        else:\n",
        "            # Use median depth of points in bounding box for robustness\n",
        "            bbox_points = projected_points[mask]\n",
        "            bbox_world_points = world_points[mask]\n",
        "\n",
        "            median_depth_idx = np.argsort(bbox_points[:, 2])[len(bbox_points) // 2]\n",
        "            detection.center_3d = bbox_world_points[median_depth_idx, :3]\n",
        "            detection.distance = bbox_points[median_depth_idx, 2]\n",
        "\n",
        "        return detection\n",
        "\n",
        "    def transform_to_global(self, detections: List[Detection],\n",
        "                           gps_data: Dict[str, float]) -> List[Detection]:\n",
        "        \"\"\"Transform detection coordinates to global GPS coordinates.\"\"\"\n",
        "        lat0, lon0, alt0 = gps_data['lat'], gps_data['lon'], gps_data['alt']\n",
        "        heading0 = gps_data['yaw']\n",
        "\n",
        "        for detection in detections:\n",
        "            if detection.center_3d is not None:\n",
        "                x, y, z = detection.center_3d\n",
        "\n",
        "                # Convert to range, azimuth, elevation\n",
        "                rng = np.sqrt(x**2 + y**2 + z**2)\n",
        "                az = np.degrees(np.arctan2(y, x)) + np.degrees(heading0)\n",
        "                el = np.degrees(np.arctan2(np.sqrt(x**2 + y**2), z)) + 90\n",
        "\n",
        "                # Convert to geodetic coordinates\n",
        "                lat, lon, alt = pm.aer2geodetic(az, el, rng, lat0, lon0, alt0)\n",
        "                detection.gps_coords = (lat, lon, alt)\n",
        "\n",
        "        return detections"
      ],
      "metadata": {
        "id": "L6iGar2qOS9R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizationEngine:\n",
        "    \"\"\"Handles visualization of fusion results.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_detections(image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        \"\"\"Draw detections on image.\"\"\"\n",
        "        vis_image = image.copy()\n",
        "\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2 = detection.bbox.astype(int)\n",
        "\n",
        "            # Draw bounding box\n",
        "            color = (0, 255, 0) if detection.distance else (255, 0, 0)\n",
        "            cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Draw label\n",
        "            label = f\"{detection.class_name}: {detection.confidence:.2f}\"\n",
        "            if detection.distance:\n",
        "                label += f\" ({detection.distance:.1f}m)\"\n",
        "\n",
        "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
        "            cv2.rectangle(vis_image, (x1, y1 - label_size[1] - 10),\n",
        "                         (x1 + label_size[0], y1), color, -1)\n",
        "            cv2.putText(vis_image, label, (x1, y1 - 5),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "        return vis_image\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_lidar_overlay(image: np.ndarray, projected_points: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Draw LiDAR points overlay on image.\"\"\"\n",
        "        vis_image = image.copy()\n",
        "\n",
        "        for point in projected_points:\n",
        "            u, v, depth = point\n",
        "            if depth > 0:\n",
        "                # Color code by depth\n",
        "                color_intensity = min(255, int(255 * (1.0 - depth / 50.0)))\n",
        "                color = (color_intensity, 0, 255 - color_intensity)\n",
        "                cv2.circle(vis_image, (int(u), int(v)), 2, color, -1)\n",
        "\n",
        "        return vis_image"
      ],
      "metadata": {
        "id": "2pMl2knpOZAN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FusionSystem:\n",
        "    \"\"\"Main fusion system orchestrator.\"\"\"\n",
        "\n",
        "    def __init__(self, config_path: Optional[str] = None):\n",
        "        self.config = ConfigManager(config_path)\n",
        "        self.performance_monitor = PerformanceMonitor()\n",
        "\n",
        "        # Initialize components\n",
        "        self.detector = None\n",
        "        self.fusion_engine = None\n",
        "        self.data_loader = None\n",
        "        self.calibration = None\n",
        "\n",
        "        logger.info(\"LiDAR-Camera Fusion System initialized\")\n",
        "\n",
        "    def initialize(self, data_path: str, calib_path: str):\n",
        "        \"\"\"Initialize system with data and calibration paths.\"\"\"\n",
        "        try:\n",
        "            # Load calibration data\n",
        "            calib_loader = CalibrationLoader(calib_path)\n",
        "            self.calibration = calib_loader.load_calibration()\n",
        "\n",
        "            # Initialize data loader\n",
        "            self.data_loader = DataLoader(data_path)\n",
        "\n",
        "            # Initialize detector\n",
        "            self.detector = ObjectDetector(self.config)\n",
        "\n",
        "            # Initialize fusion engine\n",
        "            self.fusion_engine = FusionEngine(self.config, self.calibration)\n",
        "\n",
        "            logger.info(\"System initialization completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"System initialization failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_frame(self, frame_idx: int) -> Tuple[List[Detection], np.ndarray]:\n",
        "        \"\"\"Process a single frame through the complete pipeline.\"\"\"\n",
        "        with self.performance_monitor.measure_time(\"total_frame_processing\"):\n",
        "            # Load sensor data\n",
        "            with self.performance_monitor.measure_time(\"data_loading\"):\n",
        "                sensor_frame = self.data_loader.load_frame(frame_idx)\n",
        "\n",
        "            # Object detection\n",
        "            with self.performance_monitor.measure_time(\"object_detection\"):\n",
        "                detections = self.detector.detect(sensor_frame.image)\n",
        "\n",
        "            # LiDAR-Camera fusion\n",
        "            with self.performance_monitor.measure_time(\"sensor_fusion\"):\n",
        "                fused_detections = self.fusion_engine.fuse_detections(\n",
        "                    detections, sensor_frame.lidar_points, sensor_frame.image.shape\n",
        "                )\n",
        "\n",
        "            # Global coordinate transformation\n",
        "            with self.performance_monitor.measure_time(\"coordinate_transformation\"):\n",
        "                final_detections = self.fusion_engine.transform_to_global(\n",
        "                    fused_detections, sensor_frame.gps_data\n",
        "                )\n",
        "\n",
        "            # Visualization\n",
        "            with self.performance_monitor.measure_time(\"visualization\"):\n",
        "                vis_image = VisualizationEngine.draw_detections(\n",
        "                    sensor_frame.image, final_detections\n",
        "                )\n",
        "\n",
        "            # Log metrics\n",
        "            self.performance_monitor.log_detection_count(len(final_detections))\n",
        "\n",
        "            return final_detections, vis_image\n",
        "\n",
        "    def process_sequence(self, start_frame: int = 0, num_frames: int = 10) -> Dict[str, Any]:\n",
        "        \"\"\"Process a sequence of frames.\"\"\"\n",
        "        results = []\n",
        "\n",
        "        logger.info(f\"Processing {num_frames} frames starting from frame {start_frame}\")\n",
        "\n",
        "        for i in range(start_frame, min(start_frame + num_frames, self.data_loader.num_frames)):\n",
        "            try:\n",
        "                detections, vis_image = self.process_frame(i)\n",
        "                results.append({\n",
        "                    'frame_id': i,\n",
        "                    'detections': detections,\n",
        "                    'visualization': vis_image,\n",
        "                    'timestamp': time.time()\n",
        "                })\n",
        "\n",
        "                if i % 10 == 0:\n",
        "                    logger.info(f\"Processed frame {i}/{start_frame + num_frames - 1}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to process frame {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Generate summary report\n",
        "        summary = self._generate_sequence_summary(results)\n",
        "        logger.info(f\"Sequence processing completed. {len(results)} frames processed successfully\")\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'summary': summary,\n",
        "            'performance_report': self.performance_monitor.get_performance_report()\n",
        "        }\n",
        "\n",
        "    def _generate_sequence_summary(self, results: List[Dict]) -> Dict[str, Any]:\n",
        "        \"\"\"Generate summary statistics for processed sequence.\"\"\"\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        total_detections = sum(len(r['detections']) for r in results)\n",
        "        detection_counts = [len(r['detections']) for r in results]\n",
        "\n",
        "        # Count detections by class\n",
        "        class_counts = {}\n",
        "        distance_stats = []\n",
        "\n",
        "        for result in results:\n",
        "            for detection in result['detections']:\n",
        "                class_counts[detection.class_name] = class_counts.get(detection.class_name, 0) + 1\n",
        "                if detection.distance:\n",
        "                    distance_stats.append(detection.distance)\n",
        "\n",
        "        return {\n",
        "            'total_frames': len(results),\n",
        "            'total_detections': total_detections,\n",
        "            'avg_detections_per_frame': np.mean(detection_counts),\n",
        "            'detection_by_class': class_counts,\n",
        "            'distance_statistics': {\n",
        "                'mean_distance': np.mean(distance_stats) if distance_stats else 0,\n",
        "                'median_distance': np.median(distance_stats) if distance_stats else 0,\n",
        "                'max_distance': np.max(distance_stats) if distance_stats else 0,\n",
        "                'min_distance': np.min(distance_stats) if distance_stats else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def run_real_time_simulation(self, fps: float = 10.0):\n",
        "        \"\"\"Simulate real-time processing.\"\"\"\n",
        "        frame_interval = 1.0 / fps\n",
        "\n",
        "        logger.info(f\"Starting real-time simulation at {fps} FPS\")\n",
        "\n",
        "        try:\n",
        "            for frame_idx in range(self.data_loader.num_frames):\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Process frame\n",
        "                detections, vis_image = self.process_frame(frame_idx)\n",
        "\n",
        "                # Display results (in real implementation, this would go to display/dashboard)\n",
        "                print(f\"Frame {frame_idx}: {len(detections)} detections\")\n",
        "                for det in detections[:3]:  # Show first 3 detections\n",
        "                    print(f\"  {det.class_name}: {det.confidence:.2f}, \"\n",
        "                          f\"distance: {det.distance:.1f}m\" if det.distance else \"distance: N/A\")\n",
        "\n",
        "                # Maintain frame rate\n",
        "                processing_time = time.time() - start_time\n",
        "                sleep_time = max(0, frame_interval - processing_time)\n",
        "\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time)\n",
        "                else:\n",
        "                    logger.warning(f\"Frame {frame_idx} processing exceeded target time: \"\n",
        "                                 f\"{processing_time:.3f}s > {frame_interval:.3f}s\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"Real-time simulation stopped by user\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Real-time simulation failed: {e}\")\n",
        "\n",
        "    def benchmark_performance(self, num_iterations: int = 100) -> Dict[str, Any]:\n",
        "        \"\"\"Run performance benchmarks.\"\"\"\n",
        "        logger.info(f\"Running performance benchmark with {num_iterations} iterations\")\n",
        "\n",
        "        # Warm up\n",
        "        for _ in range(5):\n",
        "            self.process_frame(0)\n",
        "\n",
        "        # Benchmark\n",
        "        benchmark_results = []\n",
        "        for i in range(num_iterations):\n",
        "            frame_idx = i % self.data_loader.num_frames\n",
        "            start_time = time.time()\n",
        "\n",
        "            detections, _ = self.process_frame(frame_idx)\n",
        "\n",
        "            processing_time = time.time() - start_time\n",
        "            benchmark_results.append({\n",
        "                'iteration': i,\n",
        "                'frame_idx': frame_idx,\n",
        "                'processing_time': processing_time,\n",
        "                'num_detections': len(detections)\n",
        "            })\n",
        "\n",
        "        # Analyze results\n",
        "        processing_times = [r['processing_time'] for r in benchmark_results]\n",
        "\n",
        "        return {\n",
        "            'num_iterations': num_iterations,\n",
        "            'mean_processing_time': np.mean(processing_times),\n",
        "            'std_processing_time': np.std(processing_times),\n",
        "            'min_processing_time': np.min(processing_times),\n",
        "            'max_processing_time': np.max(processing_times),\n",
        "            'fps_capability': 1.0 / np.mean(processing_times),\n",
        "            'percentiles': {\n",
        "                '50th': np.percentile(processing_times, 50),\n",
        "                '95th': np.percentile(processing_times, 95),\n",
        "                '99th': np.percentile(processing_times, 99)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def save_results(self, results: Dict[str, Any], output_dir: str):\n",
        "        \"\"\"Save processing results to disk.\"\"\"\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save summary as JSON\n",
        "        summary_path = output_path / 'processing_summary.json'\n",
        "        with open(summary_path, 'w') as f:\n",
        "            # Convert numpy types to Python types for JSON serialization\n",
        "            json_summary = self._convert_numpy_types(results['summary'])\n",
        "            json.dump(json_summary, f, indent=2)\n",
        "\n",
        "        # Save performance report\n",
        "        perf_path = output_path / 'performance_report.json'\n",
        "        with open(perf_path, 'w') as f:\n",
        "            perf_report = self._convert_numpy_types(results['performance_report'])\n",
        "            json.dump(perf_report, f, indent=2)\n",
        "\n",
        "        # Save detection results as CSV\n",
        "        if 'results' in results:\n",
        "            detections_data = []\n",
        "            for result in results['results']:\n",
        "                for detection in result['detections']:\n",
        "                    row = {\n",
        "                        'frame_id': result['frame_id'],\n",
        "                        'class_name': detection.class_name,\n",
        "                        'confidence': detection.confidence,\n",
        "                        'bbox_x1': detection.bbox[0],\n",
        "                        'bbox_y1': detection.bbox[1],\n",
        "                        'bbox_x2': detection.bbox[2],\n",
        "                        'bbox_y2': detection.bbox[3],\n",
        "                        'distance': detection.distance,\n",
        "                        'timestamp': detection.timestamp\n",
        "                    }\n",
        "\n",
        "                    if detection.center_3d is not None:\n",
        "                        row.update({\n",
        "                            'center_3d_x': detection.center_3d[0],\n",
        "                            'center_3d_y': detection.center_3d[1],\n",
        "                            'center_3d_z': detection.center_3d[2]\n",
        "                        })\n",
        "\n",
        "                    if detection.gps_coords is not None:\n",
        "                        row.update({\n",
        "                            'gps_lat': detection.gps_coords[0],\n",
        "                            'gps_lon': detection.gps_coords[1],\n",
        "                            'gps_alt': detection.gps_coords[2]\n",
        "                        })\n",
        "\n",
        "                    detections_data.append(row)\n",
        "\n",
        "            df = pd.DataFrame(detections_data)\n",
        "            df.to_csv(output_path / 'detections.csv', index=False)\n",
        "\n",
        "        logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "    def _convert_numpy_types(self, obj):\n",
        "        \"\"\"Convert numpy types to Python types for JSON serialization.\"\"\"\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: self._convert_numpy_types(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [self._convert_numpy_types(item) for item in obj]\n",
        "        else:\n",
        "            return obj"
      ],
      "metadata": {
        "id": "uzfHiyPIOlni"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage and testing\n",
        "def main():\n",
        "    \"\"\"Main function demonstrating system usage.\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'detection': {\n",
        "            'model_name': 'yolov5s',\n",
        "            'confidence_threshold': 0.3,\n",
        "            'iou_threshold': 0.25\n",
        "        },\n",
        "        'fusion': {\n",
        "            'max_distance_threshold': 80.0,\n",
        "            'point_cluster_eps': 0.3,\n",
        "            'min_cluster_size': 5\n",
        "        },\n",
        "        'performance': {\n",
        "            'use_gpu': True,\n",
        "            'num_threads': 4\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save example config\n",
        "    with open('fusion_config.yaml', 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "\n",
        "    # Initialize system\n",
        "    fusion_system = FusionSystem('fusion_config.yaml')\n",
        "\n",
        "    # Example data paths (adjust for your setup)\n",
        "    data_path = \"2011_10_03/2011_10_03_drive_0047_sync\"\n",
        "    calib_path = \"2011_10_03\"\n",
        "\n",
        "    try:\n",
        "        # Initialize with data\n",
        "        fusion_system.initialize(data_path, calib_path)\n",
        "\n",
        "        # Process a sequence of frames\n",
        "        results = fusion_system.process_sequence(start_frame=0, num_frames=20)\n",
        "\n",
        "        # Save results\n",
        "        fusion_system.save_results(results, 'output')\n",
        "\n",
        "        # Run performance benchmark\n",
        "        benchmark_results = fusion_system.benchmark_performance(num_iterations=50)\n",
        "        print(\"\\nPerformance Benchmark Results:\")\n",
        "        print(f\"Mean processing time: {benchmark_results['mean_processing_time']:.4f}s\")\n",
        "        print(f\"FPS capability: {benchmark_results['fps_capability']:.1f}\")\n",
        "        print(f\"95th percentile: {benchmark_results['percentiles']['95th']:.4f}s\")\n",
        "\n",
        "        # Demonstrate real-time simulation (uncomment to run)\n",
        "        # fusion_system.run_real_time_simulation(fps=5.0)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"System execution failed: {e}\")\n",
        "        return 1\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Cm0hYqEXOujL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPmpr7UEJE9G",
        "outputId": "6c732621-9bcf-4967-a37f-d816b4535c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Failed to load calibration data: [Errno 2] No such file or directory: '2011_10_03/calib_cam_to_cam.txt'\n",
            "ERROR:__main__:System initialization failed: [Errno 2] No such file or directory: '2011_10_03/calib_cam_to_cam.txt'\n",
            "ERROR:__main__:System execution failed: [Errno 2] No such file or directory: '2011_10_03/calib_cam_to_cam.txt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Calibration loading test passed\n",
            "✓ Configuration management test passed\n",
            "✓ Performance monitoring test passed\n",
            "\n",
            "All tests passed! ✓\n",
            "\n",
            "Starting main system...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Unit tests\n",
        "class TestFusionSystem:\n",
        "    \"\"\"Unit tests for the fusion system.\"\"\"\n",
        "\n",
        "    def test_calibration_loading(self):\n",
        "        \"\"\"Test calibration data loading and validation.\"\"\"\n",
        "        # Mock calibration data\n",
        "        mock_calib = CalibrationData(\n",
        "            P_rect=np.eye(3, 4),\n",
        "            R_rect=np.eye(4),\n",
        "            T_cam_velo=np.eye(4),\n",
        "            T_velo_cam=np.eye(3, 4),\n",
        "            T_cam_imu=np.eye(4),\n",
        "            camera_intrinsics={'fx': 700, 'fy': 700, 'cx': 320, 'cy': 240},\n",
        "            distortion_coeffs=np.zeros(5)\n",
        "        )\n",
        "\n",
        "        assert mock_calib.P_rect.shape == (3, 4)\n",
        "        assert mock_calib.R_rect.shape == (4, 4)\n",
        "        print(\"✓ Calibration loading test passed\")\n",
        "\n",
        "    def test_config_management(self):\n",
        "        \"\"\"Test configuration management.\"\"\"\n",
        "        config = ConfigManager()\n",
        "\n",
        "        # Test default values\n",
        "        assert config.get('detection.confidence_threshold') == 0.25\n",
        "        assert config.get('fusion.max_distance_threshold') == 100.0\n",
        "\n",
        "        # Test nested access\n",
        "        assert config.get('detection.model_name') == 'yolov5s'\n",
        "        print(\"✓ Configuration management test passed\")\n",
        "\n",
        "    def test_performance_monitor(self):\n",
        "        \"\"\"Test performance monitoring.\"\"\"\n",
        "        monitor = PerformanceMonitor()\n",
        "\n",
        "        with monitor.measure_time(\"test_operation\"):\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        report = monitor.get_performance_report()\n",
        "        assert report['total_frames_processed'] == 0\n",
        "        assert len(monitor.metrics['processing_times']) == 1\n",
        "        print(\"✓ Performance monitoring test passed\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    test_suite = TestFusionSystem()\n",
        "    test_suite.test_calibration_loading()\n",
        "    test_suite.test_config_management()\n",
        "    test_suite.test_performance_monitor()\n",
        "\n",
        "    print(\"\\nAll tests passed! ✓\")\n",
        "    print(\"\\nStarting main system...\")\n",
        "\n",
        "    # Run main system\n",
        "    exit_code = main()"
      ]
    }
  ]
}